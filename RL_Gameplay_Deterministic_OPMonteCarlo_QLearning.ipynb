{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment: Gameplay with RL\n",
    "\n",
    "- Deterministic,\n",
    "-  Off Policy monte carlo \n",
    "-  Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import relevant libraries \n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "#Define Values for parameters\n",
    "\n",
    "#Number of rounds won to win the game\n",
    "d = 3\n",
    "\n",
    "#Probability of winning given action\n",
    "ph = 0.55\n",
    "pl = 0.45\n",
    "\n",
    "#Define Cost of actions\n",
    "ch = 50\n",
    "cl = 10\n",
    "R = 1000\n",
    "\n",
    "#Two actions, high or low effort\n",
    "low = 0\n",
    "high = 1\n",
    "\n",
    "n_states = len(range(-(d),d))+1 #+1\n",
    "n_actions = 2\n",
    "\n",
    "#print(n_states)\n",
    "\n",
    "#State Value_funct_pol_high:\n",
    "\n",
    "#Build the game environment \n",
    "def game_env():\n",
    "\n",
    "    p = {}\n",
    "    grid = np.arange(n_states)\n",
    "    #print(\"grid\",grid)\n",
    "    it = np.nditer(grid, flags=['multi_index'])\n",
    "\n",
    "    with it:\n",
    "        while not it.finished: \n",
    "            s = it.iterindex\n",
    "\n",
    "\n",
    "            p[s] = {a: [] for a in range(n_actions)}\n",
    "\n",
    "            #Game ends when player is either three rounds behind or ahead\n",
    "            is_done = lambda x: x==0 or x==(n_states-1)  #x==0 or\n",
    "\n",
    "            #Define winning criteria\n",
    "            #did_win = lambda x: x==n_states\n",
    "\n",
    "            #Final reward is 1000 if win, 0 if lose\n",
    "            reward = 1000 if is_done(s) and s==6 else 0.0  #and did_win(s)  #\n",
    "\n",
    "\n",
    "            if is_done(s):\n",
    "                # Probability of ending up in the next state after action, next state, reward, done\n",
    "                # Dep on policy\n",
    "\n",
    "\n",
    "                #print(\"iterindex isdone\", s)\n",
    "                p[s][low] = [(0.5, 0, reward, True, 0.5, 0,reward, True)]       #reward()\n",
    "                p[s][high]= [(0.5, 0, reward, True, 0.5, 0,reward, True)]        #reward\n",
    "\n",
    "\n",
    "            else:\n",
    "                #If game not done, probabilities given state, action are\n",
    "\n",
    "                #print(\"iterindex else\", s)\n",
    "\n",
    "                next_s_win = s+1\n",
    "                next_s_lose= s-1\n",
    "\n",
    "                p[s][low] = [(pl, next_s_win,  reward - cl ,is_done(s),\n",
    "                              1-pl, next_s_lose, reward - cl, is_done(s))]  # change to is done if trouble :  is_done(next_s_win), is_done(next_s_lose)  #'dep on next state?'\n",
    "\n",
    "                p[s][high] = [(ph, next_s_win, reward- ch, is_done(s),\n",
    "                              1-ph, next_s_lose, reward - ch,is_done(s))]   #  'dep on next state?'\n",
    "\n",
    "            it.iternext()\n",
    "\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_policy (0, 0, 0, 0, 1, 1, 0) initial state value 281.65289007766967\n"
     ]
    }
   ],
   "source": [
    "#Define the deterministic policy evaluator. Takes a policy e.g array of actions given states as input\n",
    "\n",
    "def main_dp(policy):\n",
    "\n",
    "    #Initial state values - 0\n",
    "    state_values = np.zeros(n_states)\n",
    "\n",
    "    #Finite game\n",
    "    lam = 1.0\n",
    "\n",
    "    #Stopping criteria\n",
    "    theta = 1e-6\n",
    "\n",
    "    iteration_counter = 1\n",
    "\n",
    "    #Policy\n",
    "    #Deterministic policies\n",
    "\n",
    "    # Variables for probabilities of actions depending on input policy\n",
    "\n",
    "    transition_probs=game_env()\n",
    "    #print(\"transition_probs\", transition_probs)\n",
    "\n",
    "    #iteration\n",
    "    while True:\n",
    "        v_old = np.copy(state_values)\n",
    "        delta = 0.0\n",
    "\n",
    "\n",
    "        # Define det policy as an array which gives (action | state), alter to take this array as input\n",
    "        #Change if statement under to depend on det policy array.\n",
    "\n",
    "\n",
    "        #Evaluate all states\n",
    "        for s in range(n_states):\n",
    "\n",
    "            if policy[s]==1:\n",
    "\n",
    "                pi_h = 1\n",
    "                pi_l = 0\n",
    "\n",
    "                effort = \"high\"\n",
    "\n",
    "            elif policy[s] ==0:\n",
    "                pi_h = 0\n",
    "                pi_l = 1\n",
    "\n",
    "                effort = \"high\"\n",
    "\n",
    "            else:\n",
    "                print(\"policy error\")\n",
    "\n",
    "\n",
    "            v_s = 0.0\n",
    "\n",
    "            #Evaluate each action for each state\n",
    "            for a in range(n_actions):\n",
    "                #print(a)\n",
    "\n",
    "                #Assign policy probability to specific action iteration\n",
    "                if a==0:\n",
    "                    pi_a=pi_l\n",
    "                else:\n",
    "                    pi_a=pi_h\n",
    "\n",
    "                #Probabilities given state, action\n",
    "\n",
    "                current_entry = transition_probs[s][a][0]\n",
    "                #print(\"current_entry\", current_entry)\n",
    "\n",
    "                p_win_h = current_entry[0]\n",
    "                next_s_win = current_entry[1]   #\"s+1\"\n",
    "                reward_win = current_entry[2]\n",
    "\n",
    "                p_lose_h = current_entry[4]\n",
    "                next_s_lose = current_entry[5]  #\"s-1\"\n",
    "                reward_lose = current_entry[6]\n",
    "\n",
    "                #pi_a dep on policy, #p_sa dep on action,\n",
    "\n",
    "                #State Value function under policy (specified)\n",
    "                #Action = high/low\n",
    "                v_s += pi_a * ((p_win_h * (reward_win + lam * v_old[next_s_win])) + (p_lose_h * (reward_lose+ lam * v_old[next_s_lose])))\n",
    "\n",
    "                #print(\"s\", s, \"a\", a, current_entry, v_s )\n",
    "            state_values[s] = v_s\n",
    "            delta = np.maximum(delta, np.abs(state_values[s]-v_old[s]))\n",
    "        #if (iteration_counter-1)%10==0:\n",
    "            #print('After %s iterations(s):\\n' %iteration_counter, state_values)\n",
    "        iteration_counter +=1\n",
    "        if delta < theta:   #iteration_counter >100:       #\n",
    "            break\n",
    "    return state_values\n",
    "\n",
    "#Evaluate all policies with DP\n",
    "\n",
    "#Generate all deterministic policies\n",
    "\n",
    "def det_policies():\n",
    "    from itertools import product\n",
    "    policies = [i for i in product(range(2), repeat=n_states)]\n",
    "\n",
    "    return policies\n",
    "\n",
    "#Optimal Deterministic Policy Search\n",
    "\n",
    "def opt_det_policy_search():\n",
    "    policies = det_policies()\n",
    "\n",
    "    max_policy = [np.zeros(n_states), np.zeros(n_states)]\n",
    "\n",
    "    for policy in policies:\n",
    "        #print(policy)\n",
    "\n",
    "        policy_eval = main_dp(policy)\n",
    "        #print(\"initial state under policy\", policy, policy_eval[3])\n",
    "\n",
    "        #if policy==(1, 1, 0, 0, 0, 0, 0):\n",
    "         #   print(\"MC threshold policy\", policy_eval)\n",
    "\n",
    "        if policy_eval[3] > max_policy[1][3]:\n",
    "            max_policy = [policy, policy_eval]\n",
    "            #print(\"new max_pol\", policy, \"initial state value\", policy_eval[3])\n",
    "\n",
    "    print(\"max_policy\", max_policy[0], \"initial state value\", max_policy[1][3])\n",
    "    \n",
    "#Run optimal deterministic policy evaluation\n",
    "opt_det_policy_search()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion \n",
    "\n",
    "The DP approach finds an optimal policy of  (0, 0, 0, 0, 1, 1, 0) with initial state value of 281.65289007766967. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "Action-state values Q_sa [[   0.            0.        ]\n",
      " [  61.50747593    4.63774701]\n",
      " [ 105.9511095   148.91343199]\n",
      " [ 330.91676949  298.46770036]\n",
      " [ 511.79507628  559.82581247]\n",
      " [ 805.57254134  774.20591761]\n",
      " [1000.         1000.        ]]\n",
      "Q under threshold policy pi 0.0\n",
      "Q under threshold policy pi 4.6377470117490605\n",
      "Q under threshold policy pi 148.91343198555504\n",
      "Q under threshold policy pi 298.4677003558798\n",
      "Q under threshold policy pi 559.8258124686042\n",
      "Q under threshold policy pi 805.5725413383143\n",
      "Q under threshold policy pi 1000.0\n"
     ]
    }
   ],
   "source": [
    "#Define the threshold policy, returns binary action value low(0)  or high (1)\n",
    "def threshold_policy(s,s_star):\n",
    "\n",
    "    #If above threshold value, action is high = 1\n",
    "    if s <= s_star:\n",
    "        return 1\n",
    "    else:\n",
    "        return  0\n",
    "\n",
    "#Define behaviour policy with equiprobable actions in each state\n",
    "def behaviour_pol(s):\n",
    "\n",
    "    #Random number\n",
    "    random = np.random.rand(1)\n",
    "    #print(\"random\", random)\n",
    "    if random>0.5:\n",
    "        return 1\n",
    "\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "#Use behaviour_policy to generate an episode\n",
    "# Play game using behaviour policy until it is done\n",
    "# sample from environment\n",
    "#to keep track of t, t takes start t as an input. This is the t following the last state of the previous episode\n",
    "\n",
    "def episode_gen(start_t):\n",
    "    #Initialize game environment\n",
    "    total_space = game_env()\n",
    "\n",
    "    #Initialise s to mid match, aka 3\n",
    "    s = 3\n",
    "    done =[]\n",
    "    ep_record = []\n",
    "    r = 0\n",
    "    ep_returns = []\n",
    "\n",
    "    current_s = total_space[s] #find the current state action values\n",
    "    #print(\"current_s\", current_s, current_s[0][0][0])\n",
    "\n",
    "    #Starting t\n",
    "    t = start_t\n",
    "\n",
    "    while not done:\n",
    "        #print(\"s is\", s)\n",
    "\n",
    "        #Check if done after this one\n",
    "\n",
    "\n",
    "        done = current_s[0][0][3]\n",
    "        #print(\"done\",done)\n",
    "\n",
    "        # Generate action under under policy s\n",
    "        a = behaviour_pol(s)\n",
    "        #print(a)\n",
    "\n",
    "        game_chance = np.random.rand(1)\n",
    "        #print(game_chance)\n",
    "\n",
    "        #See what outcome the  game has\n",
    "        if game_chance < current_s[a][0][0]:\n",
    "            #print(\"win\")\n",
    "            rt = current_s[a][0][2]\n",
    "            #print(\"current_s\", current_s)\n",
    "\n",
    "            '''\n",
    "            if not done:\n",
    "                rtplus1 = total_space[s + 1][]\n",
    "            else:\n",
    "                rtplus1 = 0 \n",
    "\n",
    "            '''\n",
    "\n",
    "            # Record state, action, return\n",
    "            ep_record.append([s,a,rt,t])    #cumulative returns before?\n",
    "\n",
    "            #Add to cumulative return\n",
    "            r += rt\n",
    "\n",
    "            #Update to next s\n",
    "            s= current_s[a][0][1]\n",
    "\n",
    "            # Update current s\n",
    "            current_s = total_space[s]\n",
    "            #print(\"current_s\",s, current_s)\n",
    "\n",
    "        else:\n",
    "            #print(\"lose\")\n",
    "\n",
    "            '''\n",
    "            #Extract return at time t\n",
    "            if not done:\n",
    "                rtplus1 = total_space[s + 1]\n",
    "            else:\n",
    "                rtplus1 = 0\n",
    "            '''\n",
    "            rt = current_s[a][0][6]\n",
    "\n",
    "            # Record state, action, return\n",
    "            ep_record.append([s,a,rt,t])\n",
    "\n",
    "            # Add return\n",
    "            r += rt\n",
    "\n",
    "            # Update to next s\n",
    "            s = current_s[a][0][5]\n",
    "\n",
    "            # Update current s\n",
    "            current_s = total_space[s]\n",
    "            #print(\"current_s\",s, current_s)\n",
    "\n",
    "        #Iterate t\n",
    "        t += 1\n",
    "\n",
    "    ep_returns.append(r)\n",
    "    #ep_record.append([r])\n",
    "    #print(\"ep_record\",ep_record)\n",
    "\n",
    "    return ep_record, ep_returns\n",
    "\n",
    "#Off-policy Monte Carlo estimation\n",
    "#Base on on policy prediction\n",
    "\n",
    "def off_pol_mc_pred():\n",
    "    #Set Parameters\n",
    "    episodes = 500000 # 10000 #1000 #50000\n",
    "    s_star = 4\n",
    "    lam = 1\n",
    "\n",
    "    #Initialise values\n",
    "    returns = np.zeros([n_states])\n",
    "\n",
    "    #Episode observations under behavioural policy\n",
    "    obs_episodes = []\n",
    "\n",
    "    #Initialize action value list as multi index\n",
    "    Q_sa = np.random.randn(n_states*n_actions).reshape([n_states,n_actions])\n",
    "    #print(Q_sa)\n",
    "\n",
    "    C_sa = np.zeros(n_states*n_actions).reshape([n_states,n_actions])\n",
    "\n",
    "    start_t = 0\n",
    "\n",
    "    #Loop for all episodes\n",
    "    for e in range(episodes):\n",
    "        \n",
    "        if e%100000==0:\n",
    "            print(e)\n",
    "        \n",
    "        episode_obs, ep_returns = episode_gen(start_t)\n",
    "\n",
    "        obs_episodes.append(episode_obs)\n",
    "\n",
    "        g = 0\n",
    "        W = 1\n",
    "\n",
    "        start_t += len(episode_obs)\n",
    "        #print(e, \"ep_length\", len(episode_obs), print(ep_returns))\n",
    "\n",
    "        pi_as = 1\n",
    "        b_as = 1\n",
    "        pi_as_list = []\n",
    "        b_as_list = []\n",
    "        i = 1\n",
    "\n",
    "        #Loop for each step of the episode\n",
    "        for element in reversed(obs_episodes[e]):\n",
    "            #print(\"step\", element)\n",
    "            i += 1\n",
    "\n",
    "            #Sum returns following state of step\n",
    "            try:\n",
    "                g = lam*g + element[2] \n",
    "            except:\n",
    "                g = lam*g\n",
    "\n",
    "            s = element[0]\n",
    "            a = element[1]\n",
    "            t = element[3]\n",
    "\n",
    "            if s==6 or s==0:\n",
    "                pi_as = 1\n",
    "            else:\n",
    "\n",
    "                pi_as =  int(a==threshold_policy(s,s_star))  #probability of action given state, neither include t  #  Test policy: 0.2 if a==0 else 0.8\n",
    "                #print(s,\"tp\", a==threshold_policy(s,s_star))\n",
    "\n",
    "            b_as = 0.5  # equiprobable state\n",
    "\n",
    "            #Importance sampling weights\n",
    "\n",
    "            # C(st,at) <- C(st, at) + W\n",
    "            C_sa[s][a] += W\n",
    "            #Using Weighted importance sampling  #W <- W(pi(at|st)/b(at|st)\n",
    "            Q_sa[s][a] += W / C_sa[s][a] * (g - Q_sa[s][a])\n",
    "\n",
    "            #print(\"ep\", e, element, \"g\", g)\n",
    "            #print(\"Q\", s,a, Q_sa[s][a])  #\n",
    "            #print(\"pias\",pi_as, \"b_as\",b_as)\n",
    "\n",
    "            W = W * pi_as / b_as  # Product,   product of all action, state probabilities following\n",
    "\n",
    "            if W ==0:\n",
    "                break\n",
    "\n",
    "        #print(\"pi list\", pi_as_list, \"b list\", b_as_list)\n",
    "\n",
    "    print(\"Action-state values Q_sa\", Q_sa)\n",
    "    for s in range(n_states):\n",
    "        print(\"Q under threshold policy pi\", Q_sa[s][threshold_policy(s, s_star)])\n",
    "\n",
    "off_pol_mc_pred()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "The off policy Monte Carlo estimation under the threshold policy. A challenge in using a random policy to evaluate \n",
    "the threshold policy, which is deterministic given its threshold, is that the threshold policy probability of many of \n",
    "the sequences drop to zero whenever an action that is not strictly prescribed by the threshold policy is taken in a certain \n",
    "state. With the current behavioural policy being equiprobable, this means the probability of each state, action\n",
    "sequence being under the threshold policy is 0.5^(number of states). For each backwards iteration through an episode \n",
    "this means the probability of the last action state being in the threshold policy is 0.5, the next to last 0.5^2 etc. \n",
    "which drops exponentially as the iteration continues. This means a large number of episode observations are needed in\n",
    " order to create action value estimates for the threshold policy. It also means accuracy of the estimate could be biased\n",
    " towards the extreme values, as the states prior to either losing (s=0) or winning (s=6) will more often be used as updates\n",
    " by the importance sampling procedure.   \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "Action-state values Q_sa [[   0.            0.        ]\n",
      " [  74.50507099   15.42365236]\n",
      " [ 146.38666149  213.23475918]\n",
      " [ 369.04064858  349.27551444]\n",
      " [ 497.50829096  566.82803294]\n",
      " [ 758.08289299  766.01757565]\n",
      " [1000.         1000.        ]]\n",
      "Optimal policy p [0 0 1 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "def mc_optimal_policy():\n",
    "    #Set Parameters\n",
    "    episodes = 500000 # 10000 #1000 #50000\n",
    "    lam = 1\n",
    "\n",
    "    #Initialise values\n",
    "    returns = np.zeros([n_states])\n",
    "\n",
    "    #Episode observations under behavioural policy\n",
    "    obs_episodes = []\n",
    "\n",
    "    #Initialize action value list as multi index\n",
    "    Q_sa = np.random.randn(n_states*n_actions).reshape([n_states,n_actions])\n",
    "    #print(Q_sa)\n",
    "\n",
    "    C_sa = np.zeros(n_states*n_actions).reshape([n_states,n_actions])\n",
    "\n",
    "    start_t = 0\n",
    "    \n",
    "    # Define an approx optimal policy to be improved during control procedure\n",
    "    mc_opt_policy = np.random.choice(2, n_states)\n",
    "\n",
    "    #Loop for all episodes\n",
    "    for e in range(episodes):\n",
    "        \n",
    "        if e%100000==0:\n",
    "            print(e)\n",
    "        \n",
    "        episode_obs, ep_returns = episode_gen(start_t)\n",
    "\n",
    "        obs_episodes.append(episode_obs)\n",
    "\n",
    "        g = 0\n",
    "        W = 1\n",
    "\n",
    "        start_t += len(episode_obs)\n",
    "        #print(e, \"ep_length\", len(episode_obs), print(ep_returns))\n",
    "\n",
    "        pi_as = 1\n",
    "        b_as = 1\n",
    "        pi_as_list = []\n",
    "        b_as_list = []\n",
    "        i = 1\n",
    "\n",
    "        #Loop for each step of the episode\n",
    "        for element in reversed(obs_episodes[e]):\n",
    "            #print(\"step\", element)\n",
    "            i += 1\n",
    "\n",
    "            #Sum returns following state of step\n",
    "            g = lam*g + element[2] \n",
    "            s = element[0]\n",
    "            a = element[1]\n",
    "            t = element[3]\n",
    "\n",
    "\n",
    "            b_as = 0.5  # equiprobable state\n",
    "\n",
    "            #Importance sampling weights\n",
    "\n",
    "            # C(st,at) <- C(st, at) + W\n",
    "            C_sa[s][a] += W\n",
    "            #Using Weighted importance sampling  #W <- W(pi(at|st)/b(at|st)\n",
    "            Q_sa[s][a] += W / C_sa[s][a] * (g - Q_sa[s][a])\n",
    "\n",
    "            #print(\"ep\", e, element, \"g\", g)\n",
    "            #print(\"Q\", s,a, Q_sa[s][a])  #\n",
    "            #print(\"pias\",pi_as, \"b_as\",b_as)\n",
    "            \n",
    "            # Policy improvement: in s, ppolicy_s =  a from argmax_a(Q_sa)\n",
    "            mc_opt_policy[s] = np.argmax(Q_sa[s])\n",
    "\n",
    "            if a!= mc_opt_policy[s]:\n",
    "                break\n",
    "            \n",
    "            W = W * 1/ b_as  # Product,   product of all action, state probabilities following\n",
    "          \n",
    "    print(\"Action-state values Q_sa\", Q_sa)\n",
    "    print(\"Optimal policy p\",mc_opt_policy)\n",
    "\n",
    "\n",
    "mc_optimal_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion \n",
    "\n",
    "The off-Policy Monte Carlo algorithm for optimal policy selection above is based on the incremental implementation of Off-Poliy MC Control in  Sutton & Barto (2018 p.111). Compared to the DP approach where transition probabilities were known, the MC algorithm finds higher state action values for the actions in the initial state ( 332.96043112 & 389.75165211) than the state value of the initial state under DP (281.65). The MC Optimal policy is also different, (0 0 0 1 0 1 0) rather than (0, 0, 0, 0, 1, 1, 0). However, these policies are quite close, with the difference being optimal actions in state won 1 and 2 being swapped. Seeing that this estimate was done without knowledge of the environment, just from sampling returns, it shows how off policy MC control methods can be effective at finding approximate policies when the environment is unknown. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P2.1: Q-learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q -learning algorithm \n",
    "\n",
    "#Epislon greedy policy from class 9.\n",
    "def epsilon_greedy_policy(q_values, epsilon=0.1):  \n",
    "    if np.random.binomial(1, epsilon) == 1:  \n",
    "        return np.random.choice(actions)  \n",
    "    else:  \n",
    "        return np.random.choice([action_ for action_, value_ in enumerate(q_values) if value_ == np.max(q_values)])\n",
    "\n",
    "\n",
    "#def Q_learn(): \n",
    "\n",
    "#Set Parameters\n",
    "epsilon = 0.1\n",
    "episodes =  500000\n",
    "alpha = 0.1\n",
    "lam = 1\n",
    "\n",
    "#Initialise arbitrary Q\n",
    "Q_sa = np.random.randn(n_states*n_actions).reshape([n_states,n_actions])\n",
    "\n",
    "q_opt_policy = np.zeros(n_states)\n",
    "\n",
    "#Initialise state\n",
    "s = 3 \n",
    "\n",
    "#Initialise game environment\n",
    "total_space = game_env()\n",
    "\n",
    "for e in range(episodes): \n",
    "\n",
    "    if e%100000==0:\n",
    "        print(\"episode\", e)\n",
    "\n",
    "    done = False\n",
    "    \n",
    "    while not done:    \n",
    "        current_s = total_space[s]\n",
    "\n",
    "\n",
    "        #Take action, (implement eps greedy)\n",
    "        a =  epsilon_greedy_policy(Q_sa[s]) #np.argmax(Q_sa[s])\n",
    "        #print(\"a\", a)\n",
    "\n",
    "        #print(current_s, current_s[0])\n",
    "        game_chance = np.random.rand(1)\n",
    "\n",
    "        #Observe what outcome the  game has\n",
    "        if game_chance < current_s[a][0][0]:\n",
    "\n",
    "            #print(\"win\")\n",
    "            rt = current_s[a][0][2]\n",
    "\n",
    "            #Update to next s\n",
    "            s_next= current_s[a][0][1]\n",
    "\n",
    "        else:\n",
    "            #print(\"lose\")\n",
    "\n",
    "            rt = current_s[a][0][6]\n",
    "\n",
    "            # Update to next s\n",
    "            s_next = current_s[a][0][5]\n",
    "\n",
    "\n",
    "        #Update Q_sa\n",
    "\n",
    "        Q_sa[s][a] += alpha*(rt +lam*Q_sa[s_next][np.argmax(Q_sa[s])] - Q_sa[s][a])\n",
    "\n",
    "        #Update optimal policy\n",
    "        q_opt_policy[s] = np.argmax(Q_sa[s])\n",
    "\n",
    "        #Update s \n",
    "        s = s_next\n",
    "\n",
    "        done = current_s[0][0][3]\n",
    "        \n",
    "        if s==6 or s==0:\n",
    "            break \n",
    "\n",
    "print(\"Q\", Q_sa, \"Q policy\", q_opt_policy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "934d8f73f645cae63aa0157683889218fa9c08e7a49d8aa6445ed6e035209bee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
